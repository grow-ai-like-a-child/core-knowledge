# Core Knowledge Deficits in Multi-Modal Language Models
Official Codebase for ICML 2025 paper.
>[__"Core Knowledge Deficits in Multi-Modal Language Models"__](https://arxiv.org/abs/2410.10855)<br>
>Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Dezhi Luo, Hokin Deng<br>
[`[Paper]`](https://arxiv.org/abs/2410.10855) [`[Code]`](https://github.com/williamium3000/core-knowledge) [`[Dataset (To be release)]`]() [`[Website]`](https://williamium3000.github.io/core-knowledge/)

***Abstract.***
> While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild still lags behind humans and exhibits diminished efficacy on simple tasks that are
intuitive for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge—rudimentary cognitive abilities innate to humans from early childhood. To probe core knowledge representation in MLLMs, we draw from developmental cognitive sciences
and develop a large-scale benchmark, CoreCognition dataset, encompassing 12 core cognitive concepts. We evaluate 219 models with 10 different prompts, leading to a total of 2409 data points for analysis. Our findings reveal core knowledge
deficits in early-developed core abilities while models demonstrate human-comparable performance in high-level cognition. Moreover, we find
that low-level abilities show little to no scaling, in stark contrast to high-level abilities. Finally, we introduce an evaluation technique “Concept Hacking”, through which we demonstrate that MLLMs do not genuinely advance toward core knowledge but instead rely on illusory understanding and shortcut learning as they scale.

## Dataset

To be released


## Results
To be released

### Reproduce
To be released


