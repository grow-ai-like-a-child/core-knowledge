# Core Knowledge Deficits in Multi-Modal Language Models
Official Codebase for ICML 2025 paper.
>[__"Core Knowledge Deficits in Multi-Modal Language Models"__](https://arxiv.org/abs/2410.10855)<br>
>Yijiang Li, Qingying Gao, Tianwei Zhao, Bingyang Wang, Haoran Sun, Haiyun Lyu, Dezhi Luo, Hokin Deng<br>
[`[Paper]`](https://arxiv.org/abs/2410.10855) [`[Code]`](https://github.com/williamium3000/core-knowledge) [`[Dataset]`](https://huggingface.co/datasets/williamium/CoreCognition) [`[Website]`](https://williamium3000.github.io/core-knowledge/)

***Abstract.***
> While Multi-modal Large Language Models (MLLMs) demonstrate impressive abilities over high-level perception and reasoning, their robustness in the wild remains limited, often falling short on tasks that are intuitive and effortless for humans. We examine the hypothesis that these deficiencies stem from the absence of core knowledge--rudimentary cognitive abilities innate to humans from early childhood. To explore the core knowledge representation in MLLMs, we introduce CoreCognition, a large-scale benchmark encompassing 12 core knowledge concepts grounded in developmental cognitive science. We evaluate 230 models with 11 different prompts, leading to a total of 2,530 data points for analysis. Our experiments uncover four key findings, collectively demonstrating core knowledge deficits in MLLMs: they consistently underperform and show reduced, or even absent, scalability on low-level abilities relative to high-level ones. Finally, we propose Concept Hacking, a novel controlled evaluation method that reveals MLLMs fail to progress toward genuine core knowledge understanding, but instead rely on shortcut learning as they scale.

![Paper Poster](assets/poster.jpg)


